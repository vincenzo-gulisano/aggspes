{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook can be used to generate the plots found in the paper, once the experiments have been run**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define imports and general configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import warnings\n",
    "import matplotlib.patches as mpatches\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "plt.rcParams[\"legend.markerscale\"] = 0.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the indexes (as fraction of 1) for warm up and cool down.\n",
    "E.g., 0.1 and 0.9 mean the first 10% of the data is discarded (warm up) and the last 10% of the data is discarded (cool down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wu_i = 0.1 # Warm Up Index\n",
    "cd_i = 0.9 # Cool Down Index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifies number of repetitions (for each experiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reps = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify which statistics should be plotted. You can choose between any subset of ['injectionRate', 'throughput', 'latency', 'outputRate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = ['injectionRate', 'throughput', 'latency', 'outputRate']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify which operators are being evaluated, you can choose any subset of ['flatmap', 'join']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops = ['flatmap','join']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now specify, for each operator, which experiments IDs have been run. Notice the following IDs are valid: 'LLF', 'ALF', 'HLF', 'LHF', 'AHF', 'HHF', 'llf', 'alf', 'hlf', 'lhf', 'ahf', 'hhf' for flatmap, and 'LLJ', 'ALJ', 'HLJ', 'LHJ', 'AHJ', 'HHJ', 'llj', 'alj', 'hlj', 'lhj', 'ahj', 'hhj' for join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments_per_operator = {'join': ['LLJ', 'ALJ', 'HLJ', 'LHJ', 'AHJ', 'HHJ'], 'flatmap': ['LLF', 'ALF', 'HLF', 'LHF', 'AHF', 'HHF']} # High-end server\n",
    "# experiments_per_operator = {'join': ['llj', 'alj', 'hlj', 'lhj', 'ahj', 'hhj'], 'flatmap': ['llf', 'alf', 'hlf', 'lhf', 'ahf', 'hhf']} # Odroid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the base folder containing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = '/home/vincenzo/aggspes/java/data/output_files/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is a support function that reads the log file produced by the experiments scripts. The format is predefined, so this function should not be modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_exp_log(logfile):\n",
    "    headers = ['folder', 'repetition', 'rate', 'sleep', 'outcome']\n",
    "    out = pd.read_csv(logfile, header=None, names=headers)\n",
    "    out['repetition'].iloc[1:] = pd.to_numeric(out['repetition'].iloc[1:])\n",
    "    out['rate'].iloc[1:] = pd.to_numeric(out['rate'].iloc[1:])\n",
    "    out['sleep'].iloc[1:] = pd.to_numeric(out['sleep'].iloc[1:])\n",
    "    out['outcome'].iloc[1:] = pd.to_numeric(out['outcome'].iloc[1:])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "stats_ylabels = {'injectionRateflatmap': r'Injection Rate ($10^3$ t/s)', 'throughputflatmap': r'Throughput ($10^3$ t/s)', 'latencyflatmap': 'Latency (s)', 'outputRateflatmap': r'Output Rate ($10^3$ t/s)',\n",
    "    'injectionRatejoin': r'Injection Rate ($10^3$ t/s)', 'throughputjoin': r'Throughput ($10^3$ c/s)', 'latencyjoin': 'Latency (s)', 'outputRatejoin': r'Output Rate ($10^3$ t/s)'}\n",
    "\n",
    "\n",
    "throughput_metric_per_operator = {'join': r'$10^3$ c/s', 'flatmap': r'$10^3$ t/s'}\n",
    "\n",
    "# This are the labels we are going to use in the different plots and for the managing of data to create\n",
    "# the plots\n",
    "implementations_labels = ['native', 'aggbased1', 'aggbasedM']\n",
    "# These maps our labels with those used in Java\n",
    "# implementations_ids = {'native': 'native',\n",
    "#                        'aggbased1': 'aggbased', 'aggbasedM': 'aggbased'}\n",
    "implementations_enums = {'native': 'NATIVE',\n",
    "                       'aggbased1': 'SINGLEOUT', 'aggbasedM': 'MULTIOUT'}\n",
    "# These maps our labels with colors for plots\n",
    "implementations_colors = {'native': 'black',\n",
    "                          'aggbased1': 'blue', 'aggbasedM': 'red'}\n",
    "implementations_legend = {'native': r'$D$', 'aggbased1': r'$A$', 'aggbasedM': r'$A^+$'}\n",
    "\n",
    "for o in range(len(ops)):\n",
    "\n",
    "    operator = ops[o]\n",
    "    experiments = experiments_per_operator[operator]\n",
    "\n",
    "    data_folders = dict()\n",
    "    data_folders['native'] = base_folder + operator+'wikipedia/'\n",
    "    data_folders['aggbased1'] = base_folder + operator+'wikipedia/'\n",
    "    data_folders['aggbasedM'] = base_folder + operator+'wikipedia/'\n",
    "\n",
    "    explogs = dict()\n",
    "    # Read experiments log\n",
    "    explogs['native'] = read_exp_log(data_folders['native']+'log.txt')\n",
    "    explogs['aggbased1'] = read_exp_log(data_folders['aggbased1']+'log.txt')\n",
    "    explogs['aggbasedM'] = read_exp_log(data_folders['aggbasedM']+'log.txt')\n",
    "\n",
    "    # Add keys to overall_data\n",
    "    overall_data = dict()\n",
    "    for statistic in stats:\n",
    "        for implementation in implementations_labels:\n",
    "            overall_data[statistic+'_'+implementation] = []\n",
    "\n",
    "    for repetition in range(0, reps):\n",
    "\n",
    "        # Add keys to repetition_best_result_data\n",
    "        repetition_best_result_data = dict()\n",
    "        for statistic in stats:\n",
    "            for implementation in implementations_labels:\n",
    "                repetition_best_result_data[statistic+'_'+implementation] = []\n",
    "\n",
    "        # repetition_all_results_data is going to be plotted by repetition, by experiment, and by statistic\n",
    "        pd_data = pd.DataFrame(columns=[\n",
    "                               'repetition', 'experiment', 'implementation_type', 'stat', 'valueMean', 'valueQ99', 'outcome'])\n",
    "\n",
    "        for i in range(len(stats)):\n",
    "\n",
    "            statistic = stats[i]\n",
    "\n",
    "            for experiment in experiments:\n",
    "\n",
    "                for implementation in implementations_labels:\n",
    "\n",
    "                    # Filter on experiment and implementation type\n",
    "                    relevant_exps = explogs[implementation].loc[explogs[implementation]['folder'].str.contains(\n",
    "                        experiment+'/'+implementations_enums[implementation], case=False)]\n",
    "                    # Filter on repetition\n",
    "                    relevant_exps = relevant_exps.loc[relevant_exps['repetition'] == repetition]\n",
    "                    successful_relevant_exps = relevant_exps.loc[relevant_exps['outcome'] == 1]\n",
    "\n",
    "                    min_successful_period_throughput = successful_relevant_exps['sleep'].min()\n",
    "                    min_successful_period_latency = successful_relevant_exps['sleep'].min()\n",
    "                    #successful_relevant_exps.sort_values(by='sleep')['sleep'].iloc[2]\n",
    "\n",
    "                    # Rather than reading only the relevant one, we can read all of them and store the data to create the plot\n",
    "                    # First we sort in decreasing order for the sleep period\n",
    "                    relevant_exps = relevant_exps.sort_values(\n",
    "                        by='sleep', ascending=False)\n",
    "\n",
    "                    # Now read one by one based on the sleep period and store the data\n",
    "                    for [period, outcome] in zip(relevant_exps.sleep, relevant_exps.outcome):\n",
    "\n",
    "                        file_to_read = data_folders[implementation]+experiment+'/'+implementations_enums[implementation]+'/'+str(period)+'/'+str(\n",
    "                            repetition)+'/'+operator+'_'+statistic+'.csv'\n",
    "                        df = pd.read_csv(\n",
    "                            file_to_read, header=None, names=['ts', 'v'])\n",
    "\n",
    "                        # Compute start and end index\n",
    "                        row_count = len(df.v)\n",
    "                        start_index = int(row_count * wu_i)\n",
    "                        end_index = int(row_count * cd_i)\n",
    "\n",
    "                        # Notice -1 values are ignored. Lor latency it meas no outputs, for rates is always >= 0!\n",
    "                        temp = df.v.iloc[start_index:end_index]\n",
    "                        temp = temp.loc[temp >= 0]\n",
    "\n",
    "                        if period == min_successful_period_latency and statistic == 'latency':\n",
    "                            repetition_best_result_data[statistic + '_' + implementation].append(temp.quantile(0.99)/1000)\n",
    "                        elif period == min_successful_period_throughput and statistic != 'latency':\n",
    "                            repetition_best_result_data[statistic +'_'+implementation].append(temp.mean()/1000)\n",
    "\n",
    "                        # Storing data in pd_data\n",
    "                        new_row = {'targetRate': [(1e9/period)/1000],'repetition': [repetition], 'experiment': [experiment], 'implementation_type': [\n",
    "                            implementation],  'stat': [statistic],  'valueMean': [temp.mean()/1000],  'valueQ99': [temp.quantile(0.99)/1000], 'outcome': [outcome], 'pickedAsMaxThroughput': [1 if period == min_successful_period_throughput else 0], 'pickedAsLatency': [1 if period == min_successful_period_latency else 0]}\n",
    "                        pd_data = pd.concat([pd_data, pd.DataFrame(new_row)])\n",
    "\n",
    "            \n",
    "            this_barplot_fig, this_barplot_axs = plt.subplots(1, 1, figsize=(1.9, 2))\n",
    "            this_barplot_axs.grid(True, axis='y')\n",
    "\n",
    "            x = np.arange(len(experiments))  # the label locations\n",
    "            width = 0.333  # the width of the bars\n",
    "\n",
    "            rects1 = this_barplot_axs.bar(\n",
    "                x - width, repetition_best_result_data[statistic+'_native'], width, label=r'$D$', color=implementations_colors['native'])\n",
    "            rects2 = this_barplot_axs.bar(\n",
    "                x, repetition_best_result_data[statistic+'_aggbased1'], width, label=r'$A$', color=implementations_colors['aggbased1'])\n",
    "            rects3 = this_barplot_axs.bar(\n",
    "                x + width, repetition_best_result_data[statistic+'_aggbasedM'], width, label=r'$A^+$', color=implementations_colors['aggbasedM'])\n",
    "\n",
    "            this_barplot_axs.set_xticks(x, experiments,rotation=90)\n",
    "            this_barplot_axs.set_xlabel('High-end server')\n",
    "            this_barplot_axs.set_ylabel(stats_ylabels[statistic+operator])\n",
    "\n",
    "            # Increase the y limit for the labels to fit\n",
    "            ymin, ymax = this_barplot_axs.get_ylim()\n",
    "            this_barplot_axs.set_ylim([ymin, ymax*1.2])\n",
    "\n",
    "            this_barplot_axs.legend(ncol=3,frameon=False, borderpad=0,columnspacing=0.2,markerscale=3,loc='upper center', bbox_to_anchor=(0.5, 1.2))\n",
    "        \n",
    "            this_barplot_fig.tight_layout()\n",
    "            plt.gca().set_axisbelow(True)\n",
    "            this_barplot_fig.savefig(base_folder + operator+'_'+statistic +\n",
    "                                '_statistic_rep_'+str(repetition)+'_barplot.pdf')\n",
    "\n",
    "            for implementation in implementations_labels:\n",
    "                overall_data[statistic+'_'+implementation].append(\n",
    "                    repetition_best_result_data[statistic+'_'+implementation])\n",
    "\n",
    "        # At this point we have all the stats (throughput, latency, and outputRate) for all experiments, we can plot for this repetition and all experiments\n",
    "        for e in range(len(experiments)):\n",
    "            experiment = experiments[e]\n",
    "            global_index = (pd_data['repetition'] == repetition) & (\n",
    "                pd_data['experiment'] == experiment)\n",
    "\n",
    "            this_scalability_fig, this_scalability_axs = plt.subplots(2, 1, figsize=(3.5, 2), sharex=True, gridspec_kw={'hspace': 0})\n",
    "            this_scalability_axs[0].grid(True, axis='y')\n",
    "            this_scalability_axs[1].grid(True, axis='y')\n",
    "\n",
    "            for implementation in implementations_labels:\n",
    "\n",
    "                index = global_index & (\n",
    "                    pd_data['implementation_type'] == implementation)\n",
    "\n",
    "                # Plot injection rate vs throughput\n",
    "                # In this case I am reading targetRate instead of valueMean to plot the expected injection rate rather then the observed one\n",
    "                this_scalability_axs[0].plot(pd_data.loc[index & (pd_data['stat'] == 'injectionRate')]['targetRate'], pd_data.loc[index & (\n",
    "                    pd_data['stat'] == 'throughput')]['valueMean'], color=implementations_colors[implementation], label=implementations_legend[implementation])\n",
    "                # plot markers\n",
    "                this_scalability_axs[0].scatter(pd_data.loc[index & (pd_data['stat'] == 'injectionRate') & (\n",
    "                    pd_data['outcome'] == 1)]['targetRate'], pd_data.loc[index & (pd_data['stat'] == 'throughput') & (pd_data['outcome'] == 1)]['valueMean'], marker='^', color=implementations_colors[implementation])\n",
    "                this_scalability_axs[0].scatter(pd_data.loc[index & (pd_data['stat'] == 'injectionRate') & (\n",
    "                    pd_data['outcome'] == 0)]['targetRate'], pd_data.loc[index & (pd_data['stat'] == 'throughput') & (pd_data['outcome'] == 0)]['valueMean'], marker='v', color=implementations_colors[implementation])\n",
    "                this_scalability_axs[0].scatter(pd_data.loc[index & (pd_data['stat'] == 'injectionRate') & (\n",
    "                    pd_data['pickedAsMaxThroughput'] == 1)]['targetRate'], pd_data.loc[index & (pd_data['stat'] == 'throughput') & (pd_data['pickedAsMaxThroughput'] == 1)]['valueMean'], marker='o', color=implementations_colors[implementation])\n",
    "\n",
    "            this_scalability_axs[0].set_ylabel(r'Throughput'+'\\n'+r'('+throughput_metric_per_operator[operator]+r')')\n",
    "\n",
    "            for implementation in implementations_labels:\n",
    "\n",
    "                index = global_index & (\n",
    "                    pd_data['implementation_type'] == implementation)\n",
    "\n",
    "                # Plot injection rate vs latency\n",
    "                # In this case I am reading targetRate instead of valueMean to plot the expected injection rate rather then the observed one\n",
    "                this_scalability_axs[1].plot(pd_data.loc[index & (pd_data['stat'] == 'injectionRate')]['targetRate'], pd_data.loc[index & (\n",
    "                    pd_data['stat'] == 'latency')]['valueQ99'], color=implementations_colors[implementation], label=implementation)\n",
    "                # plot markers\n",
    "                this_scalability_axs[1].scatter(pd_data.loc[index & (pd_data['stat'] == 'injectionRate') & (\n",
    "                    pd_data['outcome'] == 1)]['targetRate'], pd_data.loc[index & (pd_data['stat'] == 'latency') & (pd_data['outcome'] == 1)]['valueQ99'], marker='^', color=implementations_colors[implementation])\n",
    "                this_scalability_axs[1].scatter(pd_data.loc[index & (pd_data['stat'] == 'injectionRate') & (\n",
    "                    pd_data['outcome'] == 0)]['targetRate'], pd_data.loc[index & (pd_data['stat'] == 'latency') & (pd_data['outcome'] == 0)]['valueQ99'], marker='v', color=implementations_colors[implementation])\n",
    "                this_scalability_axs[1].scatter(pd_data.loc[index & (pd_data['stat'] == 'injectionRate') & (\n",
    "                    pd_data['pickedAsLatency'] == 1)]['targetRate'], pd_data.loc[index & (pd_data['stat'] == 'latency') & (pd_data['pickedAsLatency'] == 1)]['valueQ99'], marker='o', color=implementations_colors[implementation])\n",
    "\n",
    "            this_scalability_axs[1].set_xlabel(r'Injection rate ($10^3$ t/s')\n",
    "\n",
    "            this_scalability_axs[1].set_ylabel(r'Latency'+'\\n'+r'(s)')\n",
    "\n",
    "            this_scalability_axs[0].legend(ncol=2,frameon=False,columnspacing=0.5,markerscale=0.5)\n",
    "\n",
    "            this_scalability_fig.tight_layout()\n",
    "            plt.gca().set_axisbelow(True)\n",
    "            this_scalability_fig.savefig(base_folder + operator + '_'+experiment+'_rep_'+str(repetition)+'_scalability.pdf')\n",
    "\n",
    "    for i in range(len(stats)):\n",
    "\n",
    "        statistic = stats[i]\n",
    "\n",
    "        this_barplot_fig, this_barplot_axs = plt.subplots(1, 1, figsize=(1.9, 2))\n",
    "        this_barplot_axs.grid(True, axis='y')\n",
    "\n",
    "        x = np.arange(len(experiments))  # the label locations\n",
    "        width = 0.333  # the width of the bars\n",
    "\n",
    "        rects1 = this_barplot_axs.bar(\n",
    "            x - width, np.mean(overall_data[statistic+'_native'], 0), width, label=r'$D$', color=implementations_colors['native'])\n",
    "        rects2 = this_barplot_axs.bar(\n",
    "            x, np.mean(overall_data[statistic+'_aggbased1'], 0), width, label=r'$A$', color=implementations_colors['aggbased1'])\n",
    "        rects3 = this_barplot_axs.bar(\n",
    "            x + width, np.mean(overall_data[statistic+'_aggbasedM'], 0), width, label=r'$A^+$', color=implementations_colors['aggbasedM'])\n",
    "\n",
    "        this_barplot_axs.set_xticks(x, experiments,rotation=90)\n",
    "        this_barplot_axs.set_xlabel('High-end server')\n",
    "        this_barplot_axs.set_ylabel(stats_ylabels[statistic+operator])\n",
    "\n",
    "        ymin, ymax = this_barplot_axs.get_ylim()\n",
    "        this_barplot_axs.set_ylim([ymin, ymax*1.2])\n",
    "\n",
    "        this_barplot_axs.legend(ncol=3,frameon=False, borderpad=0,columnspacing=0.2,markerscale=0.5,loc='upper center', bbox_to_anchor=(0.5, 1.2))\n",
    "        \n",
    "        this_barplot_fig.tight_layout()\n",
    "        plt.gca().set_axisbelow(True)\n",
    "        # this_barplot_fig.subplots_adjust(right=0.99)\n",
    "        this_barplot_fig.savefig(base_folder + operator+\"_\" +\n",
    "                            statistic+'_mean_barplot.pdf')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aggregate_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0cc6d25c82d20c31e28a70934fcef35977fd8ee557152051b1c3d0e18034f898"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
